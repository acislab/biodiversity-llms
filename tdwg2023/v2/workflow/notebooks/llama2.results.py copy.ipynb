{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process and grade LLM responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = os.path.expanduser(\"~/biodiversity-llms\")\n",
    "TAXONOMY_TESTSET_SCORES = ROOT + \"/tdwg2023/taxonomy/results/kpfg_scores.tsv\"\n",
    "TAXON_COUNTS = ROOT + \"/tdwg2023/taxonomy/results/taxon-counts.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job parameters:\n",
      "- Responses to analyze: ../../results/llama2-7b-chat/all-shuffled.tsv\n",
      "- Query phrasings (count: 6):\n",
      "    - \"Can species {genus} {specificepithet} be found in {county}, {stateprovince}, {country}?\"\n",
      "    - \"Is it possible to encounter species {genus} {specificepithet} in {county}, {stateprovince}, {country}?\"\n",
      "    - \"Is there a presence of species {genus} {specificepithet} within {county}, {stateprovince}, {country}?\"\n",
      "    - \"Does {county}, {stateprovince}, {country} harbor species {genus} {specificepithet}?\"\n",
      "    - \"Is species {genus} {specificepithet} present in {county}, {stateprovince}, {country}?\"\n",
      "    - \"Can one observe species {genus} {specificepithet} in {county}, {stateprovince}, {country}?\"\n",
      "- Query fields:\n",
      "    - \"kingdom\"\n",
      "    - \"phylum\"\n",
      "    - \"family\"\n",
      "    - \"genus\"\n",
      "    - \"specificepithet\"\n",
      "    - \"country\"\n",
      "    - \"stateprovince\"\n",
      "    - \"county\"\n"
     ]
    }
   ],
   "source": [
    "class Files(NamedTuple):\n",
    "    responses: str\n",
    "\n",
    "class Env(NamedTuple):\n",
    "    files: Files\n",
    "    num_phrasings: int\n",
    "    phrasings: list[str]\n",
    "    query_fields: list[str]\n",
    "\n",
    "if \"snakemake\" in globals():\n",
    "    env = Env(\n",
    "        files=snakemake.input,\n",
    "        num_phrasings=len(snakemake.params.phrasings),\n",
    "        phrasings=snakemake.params.phrasings,\n",
    "        query_fields=snakemake.params.query_fields\n",
    "    )\n",
    "else: # Fill in parameters manually for testing outside of snakemake\n",
    "    env = Env(\n",
    "        files = Files(\n",
    "            responses=\"../../results/llama2-7b-chat/all-shuffled.tsv\"\n",
    "        ),\n",
    "        num_phrasings=6,\n",
    "        phrasings=[\n",
    "            \"Can species {genus} {specificepithet} be found in {county}, {stateprovince}, {country}?\",\n",
    "            \"Is it possible to encounter species {genus} {specificepithet} in {county}, {stateprovince}, {country}?\",\n",
    "            \"Is there a presence of species {genus} {specificepithet} within {county}, {stateprovince}, {country}?\",\n",
    "            \"Does {county}, {stateprovince}, {country} harbor species {genus} {specificepithet}?\",\n",
    "            \"Is species {genus} {specificepithet} present in {county}, {stateprovince}, {country}?\",\n",
    "            \"Can one observe species {genus} {specificepithet} in {county}, {stateprovince}, {country}?\"\n",
    "        ],\n",
    "        query_fields=[\n",
    "            \"kingdom\",\n",
    "            \"phylum\",\n",
    "            \"family\",\n",
    "            \"genus\",\n",
    "            \"specificepithet\",\n",
    "            \"country\",\n",
    "            \"stateprovince\",\n",
    "            \"county\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def nest(level, strings):\n",
    "    separator = \"\\n\" + \"  \" * level + \"- \"\n",
    "    return separator + separator.join([str(s) for s in strings])\n",
    "\n",
    "def quote(strings):\n",
    "    return [f'\"{s}\"' for s in strings]\n",
    "\n",
    "print(\"Job parameters:\")\n",
    "print(f\"- Responses to analyze: {env.files.responses}\")\n",
    "print(f\"- Query phrasings (count: {env.num_phrasings}):{nest(2, quote(env.phrasings))}\")\n",
    "print(f\"- Query fields:{nest(2, quote(env.query_fields))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(open(env.files.responses), sep=\"\\t\").head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kingdom</th>\n",
       "      <th>phylum</th>\n",
       "      <th>family</th>\n",
       "      <th>genus</th>\n",
       "      <th>specificepithet</th>\n",
       "      <th>country</th>\n",
       "      <th>stateprovince</th>\n",
       "      <th>county</th>\n",
       "      <th>present</th>\n",
       "      <th>query</th>\n",
       "      <th>response</th>\n",
       "      <th>first token top strings</th>\n",
       "      <th>first token top scores</th>\n",
       "      <th>question number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fungi</td>\n",
       "      <td>basidiomycota</td>\n",
       "      <td>polyporaceae</td>\n",
       "      <td>Polyporus</td>\n",
       "      <td>gilvus</td>\n",
       "      <td>United States</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Sarasota County</td>\n",
       "      <td>No</td>\n",
       "      <td>'Is there a presence of species Scutellospora ...</td>\n",
       "      <td>'Yes.\\n\\nPol'</td>\n",
       "      <td>['Yes', 'No', '', '', '']</td>\n",
       "      <td>['35.208', '35.208', '-inf', '-inf', '-inf']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  kingdom         phylum        family      genus specificepithet  \\\n",
       "0   fungi  basidiomycota  polyporaceae  Polyporus          gilvus   \n",
       "\n",
       "         country stateprovince           county present  \\\n",
       "0  United States       Florida  Sarasota County      No   \n",
       "\n",
       "                                               query       response  \\\n",
       "0  'Is there a presence of species Scutellospora ...  'Yes.\\n\\nPol'   \n",
       "\n",
       "     first token top strings                        first token top scores  \\\n",
       "0  ['Yes', 'No', '', '', '']  ['35.208', '35.208', '-inf', '-inf', '-inf']   \n",
       "\n",
       "   question number  \n",
       "0                1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'responses'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/envs/llm/lib/python3.12/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:182\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'responses'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m     28\u001b[0m UNUSED_FIELDS \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponses\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput token count\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput token count\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 29\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mget_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponses\u001b[49m\u001b[43m)\u001b[49m\\\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mUNUSED_FIELDS)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Drop records with the same family and genus, they cause headaches later\u001b[39;00m\n\u001b[1;32m     33\u001b[0m res \u001b[38;5;241m=\u001b[39m res[res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenus\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m s: s\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;241m!=\u001b[39m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfamily\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m s: s\u001b[38;5;241m.\u001b[39mlower())]\n",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m, in \u001b[0;36mget_results\u001b[0;34m(responses)\u001b[0m\n\u001b[1;32m     16\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m make_id(df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphrasing\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[1;32m     17\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Drop responses for repeated questions\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponses\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m r: count_item(r\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39msplit(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     20\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myesnos\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponses\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m r: count_item(r\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39msplit(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m count_item(r\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39msplit(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     21\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabstains\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m-\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myesnos\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/mambaforge/envs/llm/lib/python3.12/site-packages/pandas/core/frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/mambaforge/envs/llm/lib/python3.12/site-packages/pandas/core/indexes/base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3805\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3806\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3807\u001b[0m     ):\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'responses'"
     ]
    }
   ],
   "source": [
    "def count_item(values, item):\n",
    "    counts = dict(Counter(values).most_common())\n",
    "    return counts[item] if item in counts else 0\n",
    "\n",
    "def make_id(df):\n",
    "    return df.apply(lambda r: hash(\"\".join([str(v) for v in r.values]).lower()), axis=1)\n",
    "\n",
    "def get_results(responses):\n",
    "    df = pd.read_csv(open(responses), sep=\"\\t\")\n",
    "    display(df.head(1))\n",
    "\n",
    "    df[\"phrasing\"] = df[\"question number\"].astype(int) % env.num_phrasings\n",
    "    df[\"target\"] = (df[\"present\"] == \"Yes\").astype(int) * 2 - 1\n",
    "    df[\"query id\"] = make_id(df[env.query_fields])\n",
    "\n",
    "    df[\"response id\"] = make_id(df[[\"query id\", \"phrasing\"]])\n",
    "    df = df.groupby(\"response id\").head(1) # Drop responses for repeated questions\n",
    "\n",
    "    df[\"first token top scores\"] = df[\"first token top scores\"].map(lambda x: eval(x))\n",
    "    df[\"first token top strings\"] = df[\"first token top strings\"].map(lambda x: [s.lower() for s in eval(x)])\n",
    "    df[\"responses\"] = df[\"first token top strings\"].map(lambda x: str(x[0]).lower())\n",
    "    df[\"no score\"] = df.apply(lambda r: dict(zip(r[\"first token top strings\"], r[\"first token top scores\"])).get(\"no\"), axis=1)\n",
    "    df[\"yes score\"] = df.apply(lambda r: dict(zip(r[\"first token top strings\"], r[\"first token top scores\"])).get(\"yes\"), axis=1)\n",
    "\n",
    "    df[\"prediction\"] = df[\"responses\"].map(lambda x: {\"yes\": 1, \"no\": -1}.get(x, 0))\n",
    "    df[\"correct\"] = df[\"prediction\"] * df[\"target\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "UNUSED_FIELDS = [\"query\", \"responses\", \"input token count\", \"output token count\"]\n",
    "res = get_results(env.files.responses)\\\n",
    "    .drop(columns=UNUSED_FIELDS)\n",
    "\n",
    "# Drop records with the same family and genus, they cause headaches later\n",
    "res = res[res[\"genus\"].apply(lambda s: s.lower()) != res[\"family\"].apply(lambda s: s.lower())]\n",
    "\n",
    "print(f\"{len(res) / env.num_phrasings:,.0f} records\")\n",
    "print(f\"{len(res):,.0f} queries (#records x #phrasings)\")\n",
    "res.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpfg_scores = pd.read_csv(open(TAXONOMY_TESTSET_SCORES, \"r\"), sep=\"\\t\")\n",
    "kpfg_scores[\"accuracy\"] = (1 + kpfg_scores[\"num_correct\"]) / (2 + kpfg_scores[\"num_response\"])\n",
    "kpfg_scores = kpfg_scores.set_index([\"subject rank\", \"taxon\"])\n",
    "\n",
    "phrasing_avg_pred = res.groupby(\"query id\")[\"prediction\"].mean()\n",
    "phrasing_var_score = res.groupby(\"query id\")[\"scores\"].var()\n",
    "record_counts_by_taxon = pd.read_csv(open(TAXON_COUNTS, \"r\"), sep=\"\\t\").set_index([\"kingdom\", \"phylum\", \"family\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = res\n",
    "df = df[(df[\"correct\"] != 0) * (df[\"phrasing\"] == 0)]\n",
    "\n",
    "def get_acc_by_field(d, field, smoothing=1):\n",
    "    num_records = d.groupby(field).size()\n",
    "    num_correct = smoothing + d[d[\"correct\"] == 1].groupby(field).size().reindex(num_records.index).fillna(0)\n",
    "    num_incorrect = smoothing + d[d[\"correct\"] == -1].groupby(field).size().reindex(num_records.index).fillna(0)\n",
    "    return num_correct / (num_correct + num_incorrect)\n",
    "\n",
    "acc_by_rank = pd.Series({rank: get_acc_by_field(df, rank) for rank in [\"kingdom\", \"phylum\", \"family\"]})\n",
    "acc_by_country = get_acc_by_field(df, \"country\")\n",
    "acc_by_stateprovince = get_acc_by_field(df, \"stateprovince\")\n",
    "\n",
    "def get_ums(instance):\n",
    "    # TODO: condition um3 on prediction?\n",
    "    um5 = record_counts_by_taxon.loc[instance[\"kingdom\"], instance[\"phylum\"], instance[\"family\"]]\n",
    "    um1_present = instance[\"scores\"]\n",
    "    um1_absent = 1 if um1_present == 0 else 0\n",
    "\n",
    "    return pd.Series({\n",
    "        \"um1_present\": um1_present,\n",
    "        \"um1_absent\": um1_absent,\n",
    "        \"um1_either\": um1_present + um1_absent,\n",
    "        \"um2\": 1.0 - instance[\"abstains\"] / 10.0,\n",
    "        \"um3_kingdom\": acc_by_rank[\"kingdom\"][instance[\"kingdom\"]],\n",
    "        \"um3_phylum\": acc_by_rank[\"phylum\"][instance[\"phylum\"]],\n",
    "        \"um3_family\": acc_by_rank[\"family\"][instance[\"family\"]],\n",
    "        \"um3_country\": acc_by_country[instance[\"country\"]],\n",
    "        \"um3_stateprovince\": acc_by_country[instance[\"country\"]],\n",
    "        \"um4_agreement\": phrasing_avg_pred[instance[\"query id\"]] * -instance[\"prediction\"],\n",
    "        \"um4_score_var\": phrasing_var_score[instance[\"query id\"]],\n",
    "        \"um5_kingdom\": um5[\"kingdomCount\"],\n",
    "        \"um5_phylum\": um5[\"phylumCount\"],\n",
    "        \"um5_family\": um5[\"familyCount\"],\n",
    "        \"um6_phylum\": kpfg_scores[\"accuracy\"][\"phylum\"][instance[\"phylum\"].lower()],\n",
    "        \"um6_family\": kpfg_scores[\"accuracy\"][\"family\"][instance[\"family\"].lower()],\n",
    "        \"um6_genus\": kpfg_scores[\"accuracy\"][\"genus\"][instance[\"genus\"].lower()],\n",
    "        \"um7_phylum\": kpfg_scores[\"num_response\"][\"phylum\"][instance[\"phylum\"].lower()] / 10,\n",
    "        \"um7_family\": kpfg_scores[\"num_response\"][\"family\"][instance[\"family\"].lower()] / 40,\n",
    "        \"um7_genus\": kpfg_scores[\"num_response\"][\"genus\"][instance[\"genus\"].lower()] / 50,\n",
    "    })\n",
    "\n",
    "train_df = pd.concat([df.apply(lambda row: get_ums(row), axis=1), df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_accuracies(df, field, title, remap_values={}, format=\"{:,.2%}\"):\n",
    "    df = pd.DataFrame(\n",
    "        (df.groupby(field)[\"correct\"].mean() * .5 + .5)\n",
    "        .rename(\"Response accuracy\")\n",
    "    ).transpose()\n",
    "    df.columns.name = title\n",
    "    df.rename(columns=remap_values, inplace=True)\n",
    "\n",
    "    display(df.style.format(format))\n",
    "\n",
    "show_accuracies(res, \"phrasing\", \"Phrasing\")\n",
    "show_accuracies(res, \"target\", \"Actual presence\", remap_values={-1: \"Absent\", 1: \"Present\"})\n",
    "show_accuracies(res, \"prediction\", \"Predicted presence\", remap_values={-1: \"Absent\", 1: \"Present\"})\n",
    "show_accuracies(res, \"kingdom\", \"Kingdom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kingdoms = [\n",
    "    \"animalia\",\n",
    "    \"plantae\",\n",
    "    # \"fungi\"\n",
    "]\n",
    "    \n",
    "test_kingdoms = [\n",
    "    \"animalia\",\n",
    "    \"plantae\",\n",
    "    \"fungi\"\n",
    "]\n",
    "\n",
    "test_fraction = .5\n",
    "\n",
    "features = [\n",
    "    \"um1_present\", # um1: scores\n",
    "    \"um1_absent\",\n",
    "    \"um1_either\",\n",
    "    \"um2\", # um2: number of abstains\n",
    "    \"um3_kingdom\", # um3: accuracy by field values\n",
    "    \"um3_phylum\",\n",
    "    # 'um3_family',\n",
    "    \"um3_country\",\n",
    "    \"um3_stateprovince\",\n",
    "    \"um4_agreement\", # Variance in scores with phrasing changes\n",
    "    \"um4_score_var\",\n",
    "    'um5_kingdom', # um5: iDigBio record counts by taxonomic ranks\n",
    "    'um5_phylum',\n",
    "    'um5_family',\n",
    "    \"um6_phylum\", # Accuracy on taxonomy questions\n",
    "    \"um6_family\",\n",
    "    \"um6_genus\",\n",
    "    \"um7_phylum\", # Number of yes-no responses to taxonomy questions\n",
    "    \"um7_family\",\n",
    "    \"um7_genus\",\n",
    "]\n",
    "\n",
    "predictions = {\n",
    "    -1,\n",
    "     1\n",
    "}\n",
    "\n",
    "only_valid_absences = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "roc_args = {\"marker\":\".\", \"linestyle\":\"--\"}\n",
    "\n",
    "if only_valid_absences and \"valid\" in train_df:\n",
    "    tdf = train_df[train_df[\"valid\"]]\n",
    "else:\n",
    "    tdf = train_df\n",
    "\n",
    "if len(train_kingdoms) > 0:\n",
    "    train_set = tdf[tdf[\"kingdom\"].isin(train_kingdoms)]\n",
    "    test_set = tdf[tdf[\"kingdom\"].isin(test_kingdoms)]\n",
    "else:\n",
    "    train_set, test_set = train_test_split(\n",
    "        tdf[tdf[\"kingdom\"].isin(test_kingdoms) * tdf[\"prediction\"].isin(predictions)],\n",
    "        test_size=test_fraction\n",
    "    )\n",
    "\n",
    "\n",
    "xgboost = HistGradientBoostingClassifier(\n",
    "    loss='log_loss',\n",
    "    # early_stopping=True,\n",
    "    min_samples_leaf=1,\n",
    "    max_iter=100,\n",
    "    monotonic_cst=np.ones_like(features, dtype=int)\n",
    ").fit(train_set[features], train_set[\"correct\"]) # categorical_features=[\"stateprovince\"]\n",
    "\n",
    "k = test_kingdoms\n",
    "f, axes = plt.subplots(len(k), 2, figsize=(10, 4 * len(k)))\n",
    "\n",
    "if len(k) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "axes[0][0].title.set_text(f\"ROC\")\n",
    "axes[0][1].title.set_text(f\"Precision-recall\")\n",
    "\n",
    "for i, ik in enumerate(k):\n",
    "    ts = test_set[test_set[\"kingdom\"] == ik]\n",
    "    probs = xgboost.predict_proba(ts[features])\n",
    "    RocCurveDisplay.from_predictions(ts[\"correct\"], probs[:,1], ax=axes[i][0], **roc_args);\n",
    "    display = PrecisionRecallDisplay.from_predictions(\n",
    "        ts[\"correct\"], probs[:,1], plot_chance_level=True, drop_intermediate=True, ax=axes[i][1], drawstyle=\"default\", **roc_args\n",
    "    )\n",
    "    axes[i][1].set_ylim(0.5, 1)\n",
    "    axes[i][1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"kingdom\"] == \"animalia\"][\"phylum\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = test_set\n",
    "ts[\"certainty\"] = xgboost.predict_proba(ts[features])[:,1]\n",
    "ts[\"correct\"] = ts[\"correct\"].clip(0,1)\n",
    "ts = ts.sort_values(\"certainty\")\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"Certainty\": ts[\"certainty\"],\n",
    "    \"Precision\": [ts[ts[\"certainty\"] >= c][\"correct\"].mean() for c in ts[\"certainty\"]],\n",
    "    \"Trust rate\": [ts[ts[\"certainty\"] >= c].index.size / ts.index.size for c in ts[\"certainty\"]]\n",
    "}).set_index(\"Certainty\").plot()\n",
    "\n",
    "# Well-calibrated reference line\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_model = (\n",
    "    \"isotonic\"\n",
    "    # \"logistic\"\n",
    ")\n",
    "calibration_set = (\n",
    "    train_set\n",
    "    # test_set\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - It would be better to calibrate on a subset, then test the calibration on the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Learn confidence function\n",
    "rs = calibration_set.copy()\n",
    "rs[\"certainty\"] = xgboost.predict_proba(rs[features])[:,1]\n",
    "rs[\"correct\"] = rs[\"correct\"].clip(0,1)\n",
    "rs = rs.sort_values(\"certainty\")\n",
    "\n",
    "if conf_model == \"isotonic\":\n",
    "    ir = IsotonicRegression(out_of_bounds=\"clip\").fit(rs[\"certainty\"].to_numpy(), rs[\"correct\"].to_numpy())\n",
    "    calibrate = lambda x: ir.transform(x) \n",
    "elif conf_model == \"logistic\":\n",
    "    lr = LogisticRegression().fit(rs[[\"certainty\"]].to_numpy(), rs[\"correct\"].to_numpy())\n",
    "    calibrate = lambda x: lr.predict_proba(x.reshape(-1, 1) if type(x) == np.ndarray else x.to_numpy().reshape(-1, 1))[:,1]\n",
    "else:\n",
    "    calibrate = lambda x: 0\n",
    "    uhoh\n",
    "\n",
    "# Plot confidence vs. precision\n",
    "ts[\"confidence\"] = calibrate(ts[\"certainty\"])\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"Confidence\": ts[\"confidence\"],\n",
    "    \"Precision\": [ts[ts[\"confidence\"] >= c][\"correct\"].mean() for c in ts[\"confidence\"]],\n",
    "    \"Trust rate\": [ts[ts[\"confidence\"] >= c].index.size / ts.index.size for c in ts[\"confidence\"]]\n",
    "}).set_index(\"Confidence\").plot()\n",
    "\n",
    "# Well-calibrated reference line\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "\n",
    "plt.xlim(0, 1.01)\n",
    "plt.ylim(0, 1.01)\n",
    "plt.legend(loc=\"center right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_args = {\n",
    "    \"n_bins\": 100,\n",
    "    \"strategy\": (\n",
    "        # \"uniform\"\n",
    "        \"quantile\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = calibration_args | { \"pos_label\": 1, \"y_true\": ts[\"correct\"] }\n",
    "cert_prob_true, cert = calibration_curve(y_prob=ts[\"certainty\"], **args)\n",
    "conf_prob_true, conf = calibration_curve(y_prob=ts[\"confidence\"], **args)\n",
    "\n",
    "print(\"Certainty ECE:\", abs(cert - cert_prob_true).sum() / len(cert))\n",
    "print(\"Confidence ECE:\", abs(conf - conf_prob_true).sum() / len(conf))\n",
    "\n",
    "plt.plot(cert, cert_prob_true, label=\"Accuracy\")\n",
    "plt.plot(cert, calibrate(cert), label=\"Confidence\")\n",
    "plt.xlabel(\"Uncertainty bins\")\n",
    "plt.xlim([0,1.01])\n",
    "plt.ylim([0,1.01])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts[[\"certainty\", \"confidence\"]].hist(bins=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
